---
title: Full title of the paper (Capitalized)
author:
  - name: Dominik Leutnant
    affil: 1,2,\ddagger,*
    orcid: 0000-0003-3293-2315
  - name: John Doe
    affil: 2, \dagger, \ddagger
affiliation:
  - num: 1
    address: |
      Muenster University of Applied Sciences - 
      Institute for Infrastructure, Water, Resources, Environment
      Correnstr. 25, 48149 Muenster, Germany
    email: leutnant@fh-muenster.de
  - num: 2
    address: |
      Your department
      Street, City, Country
    email: mail@mail.com
# author citation list in chicago format
authorcitation: |
  Leutnant, D.; Doe, J.
# firstnote to eighthnote
firstnote: |
  Current address: Updated affiliation
secondnote: |
  These authors contributed equally to this work.
correspondence: |
  leutnant@fh-muenster.de; Tel.: +XX-000-00-0000.
# document options
journal: notspecified
type: article
status: submit
# front matter
simplesummary: |
  A Simple summary goes here.
abstract: |
  A single paragraph of about 200 words maximum. For research articles, 
  abstracts should give a pertinent overview of the work. We strongly encourage
  authors to use the following style of structured abstracts, but without 
  headings: 1) Background: Place the question addressed in a broad context and
  highlight the purpose of the study; 2) Methods: Describe briefly the main
  methods or treatments applied; 3) Results: Summarize the article's main 
  findings; and 4) Conclusion: Indicate the main conclusions or interpretations. 
  The abstract should be an objective representation of the article, it must not 
  contain results which are not presented and substantiated in the main text and 
  should not exaggerate the main conclusions.
# back matter
keywords: |
  keyword 1; keyword 2; keyword 3 (list three to ten pertinent keywords specific 
  to the article, yet reasonably common within the subject discipline.).
acknowledgement: |
  All sources of funding of the study should be disclosed. Please clearly 
  indicate grants that you have received in support of your research work. 
  Clearly state if you received funds for covering the costs to publish in open 
  access.
authorcontributions: |
  For research articles with several authors, a short paragraph specifying their 
  individual contributions must be provided. The following statements should be 
  used ``X.X. and Y.Y. conceive and designed the experiments; X.X. performed the 
  experiments; X.X. and Y.Y. analyzed the data; W.W. contributed 
  reagents/materials/analysis tools; Y.Y. wrote the paper.'' Authorship must be
  limited to those who have contributed substantially to the work reported.
funding: |
  Please add: ``This research received no external funding'' or ``This research 
  was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded 
  by XXX''. Check carefully that the details given are accurate and use the 
  standard spelling of funding agency names at 
  \url{https://search.crossref.org/funding}, any errors may affect your future 
  funding.
institutionalreview: |
  In this section, you should add the Institutional Review Board Statement and 
  approval number, if relevant to your study. You might choose to exclude 
  this statement if the study did not require ethical approval. Please note 
  that the Editorial Office might ask you for further information. Please add 
  “The study was conducted in accordance with the Declaration of Helsinki, 
  and approved by the Institutional Review Board (or Ethics Committee) of 
  NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies 
  involving humans. OR “The animal study protocol was approved by the 
  Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE 
  (protocol code XXX and date of approval).” for studies involving animals. 
  OR “Ethical review and approval were waived for this study due to REASON 
  (please provide a detailed justification).” OR “Not applicable” for
   studies not involving humans or animals.
informedconsent: |
  Any research article describing a study involving humans should contain this 
  statement. Please add ``Informed consent was obtained from all subjects 
  involved in the study.'' OR ``Patient consent was waived due to REASON 
  (please provide a detailed justification).'' OR ``Not applicable'' for 
  studies not involving humans. You might also choose to exclude this statement 
  if the study did not involve humans.
  
  Written informed consent for publication must be obtained from participating 
  patients who can be identified (including by the patients themselves). Please 
  state ``Written informed consent has been obtained from the patient(s) to 
  publish this paper'' if applicable.
dataavailability: |
  We encourage all authors of articles published in MDPI journals to share 
  their research data. In this section, please provide details regarding where 
  data supporting reported results can be found, including links to publicly 
  archived datasets analyzed or generated during the study. Where no new data 
  were created, or where data is unavailable due to privacy or ethical 
  re-strictions, a statement is still required. Suggested Data Availability 
  Statements are available in section “MDPI Research Data Policies” at 
  \url{https://www.mdpi.com/ethics}.
conflictsofinterest: |
  Declare conflicts of interest or state 'The authors declare no conflict of 
  interest.' Authors must identify and declare any personal circumstances or
  interest that may be perceived as inappropriately influencing the
  representation or interpretation of reported research results. Any role of the
  funding sponsors in the design of the study; in the collection, analyses or 
  interpretation of data in the writing of the manuscript, or in the decision to 
  publish the results must be declared in this section. If there is no role, 
  please state 'The founding sponsors had no role in the design of the study; 
  in the collection, analyses, or interpretation of data; in the writing of the 
  manuscript, an in the decision to publish the results'.
sampleavailability: |
  Samples of the compounds ...... are available from the authors.
supplementary: |
 The following supporting information can be downloaded at:  
 \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.
abbreviations:
  - short: MDPI
    long: Multidisciplinary Digital Publishing Institute
  - short: DOAJ
    long: Directory of open access journals
  - short: TLA
    long: Three letter acronym
  - short: LD 
    long: linear dichroism
bibliography: mybibfile.bib
appendix: appendix.tex
endnotes: false
output: 
  rticles::mdpi_article:
    extra_dependencies: longtable
---

# Version

This Rmd-skeleton uses the mdpi Latex template published 2023-03-25. 
However, the official template gets more frequently updated than the **rticles**
package. Therefore, please make sure prior to paper submission, that you're 
using the most recent .cls, .tex and .bst files 
(available [here](http://www.mdpi.com/authors/latex)).

# Article Header Information

The YAML header includes information needed mainly for formatting the front and 
back matter of the article. Required elements include:

```yaml
title: Title of the paper
author:
  - name: first and last name
    affil: |
      One or more comma seperated numbers corresponding to affilitation
      and one or more  comma seperated symbols corresponding 
      optional notes.
    orcid: optional orcid number
affiliation:  
  - num: 1,..., n for each affiliation
    address: required
    email: required
authorcitation: |
  Lastname, F.
correspondence: |
  email@email.com; Tel.: +XX-000-00-0000.
journal: notspecified
type: article
status: submit
```

Journal options are in Table \ref{tab:mdpinames}. The `status` variable should 
generally not be changed by authors. The `type` variable describes 
the type of of submission and defaults to `article` but can be replaced with any of the ones in Table \ref{tab:mdpitype}

```{r mdpitype, echo = FALSE}
type <- c("abstract, addendum, article, book, bookreview, briefreport, 
casereport, comment, commentary, communication, conferenceproceedings, 
correction, conferencereport, entry, expressionofconcern, extendedabstract, 
datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, 
obituary, opinion, projectreport, reply, retraction, review, perspective, 
protocol, shortnote, studyprotocol, systematicreview, supfile, 
technicalnote, viewpoint, guidelines, registeredreport, tutorial")
knitr::kable(rticles::string_to_table(type, 3), 
             col.names = NULL, format = "latex", booktab = TRUE, 
             caption = "MDPI article types.")
```

## Journal Specific YAML variables

```yaml
# for journal Diversity,
# add the Life Science Identifier using:
lsid: http://zoobank.org/urn:lsid:zoobank.org:act:nnnn


# for journal Applied Sciences
# add featured application
featuredapplication: |
  Authors are encouraged to provide a concise 
  description of the specific application or 
  a potential application of the work. This 
  section is not mandatory.

# for the journal Data
# add dataset doi and license
dataset: https://doi.org/10.1000/182
datasetlicense: CC-BY-4.0

# for the journal Toxins
# add key contributions
keycontributions: |
  The breakthroughs or highlights of the manuscript. 
  Authors can write one or two sentences to describe 
  the most important part of the paper.

# for the journal Encyclopedia
encyclopediadef: |
  For entry manuscripts only: please provide a brief overview
  of the entry title instead of an abstract.
entrylink: The Link to this entry published on the encyclopedia platform.

# for the journal Advances in Respiratory Medicine
# add highlights
addhighlights: |
  This is an obligatory section in “Advances in Respiratory Medicine”, 
  whose goal is to increase the discoverability and readability of the
  article via search engines and other scholars. Highlights should not 
  be a copy of the abstract, but a simple text allowing the reader to 
  quickly and simplified find out what the article is about and what can 
  be cited from it. Each of these parts should be devoted up to 2~bullet 
  points.

```
```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
rm(list=ls())
# CONFIGURACIÓN GENERAL
library(knitr)
options(width = 100)
# Opciones generales chunks
opts_chunk$set(echo=T,message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = './figure/')

#options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
#knit_hooks$set(plot = knitr:::hook_plot_html)
```

# Carga de librerías

```{r}
library(dplyr)
library(pacman)
packages = c("dplyr","MASS","knitr","tidyverse","car",'dplyr','kableExtra',"tidyr","readr","magrittr")
pacman::p_load(char=packages)
```

# Carga de ficheros

Para crear el dataset con el que vamos a tratar en este proyecto, hemos extraido varios archivos de la [web](https://valencia.opendatasoft.com/pages/home/) del portal de datos abiertos del Ayuntamiento de Valencia. En ellos tenemos diferente información acerca de los 88 barrios que hay en Valencia, como pueden ser el número de zonas verdes, precio del alquiler, actividad comercial, renta, etc. Antes de atacar las preguntas que nuestro conjunto resolverá, vamos a cargar los datos y unirlos en un único dataset, con una variable común para todos, el barrio.

## Vulnerabilidad
El primer dataset [Vulnerabilidad](https://valencia.opendatasoft.com/explore/dataset/vulnerabilidad/table/) nos da información general del barrio, como la densidad de población, la renta media, o el estado de vulnerabilidad. Esta última variable será de gran interés en nuestro análisis posterior.

```{r}
vuln <- read_delim("./data/vulnerabilidad.csv", 
    delim = ";", escape_double = FALSE, col_types = cols(`Geo Point` = col_skip(), 
        `Geo Shape` = col_skip(), `Densitat_p` = col_skip()), 
    trim_ws = TRUE)%>%arrange(nombre)

colnames(vuln)[colnames(vuln) == "nombre"] <-"Barrio"
colnames(vuln)[colnames(vuln) == "Index_Gl_1"] <-"Indice_Vuln"


vuln$Barrio<-factor(vuln$Barrio,levels = unique(vuln$Barrio))

vuln$Indice_Vuln<-factor(vuln$Indice_Vuln,levels = c("Vulnerable","Pot. Vulnerable","No Vulnerable"))


head(vuln)
```

Podemos ver en el código que es importante transformar la variable `Barrio` en un factor, para poder graficar y tratar la información de forma adecuada. Repetiremos este proceso en cada conjunto de datos, además de poner a todos el mismo nombre para poder unirlos más adelante.

## Población

Este conjunto contiene el area y la población de los diferentes barrios. Hemos considerado este dataset debido a que los datos de areas y densidad de población que nos proporcionaba el conjunto vulnerabilidad y barrios no cuadraban con nuestras búsquedas y carecían de sentido. Por ello hemos considerado este otro que se ajusta mucho mejor. Para calcular la densidad usaremos la función mutate.

```{r}
load("./data/Demografico.RData")

colnames(demografico)[colnames(demografico) == "nombre"] <-"Barrio"

demografico$Barrio<-factor(demografico$Barrio,levels = unique(demografico$Barrio))

demografico%<>%
  arrange(`Barrio`)%>%
  mutate(across(-c("Barrio"), as.numeric))%>%
  mutate(`Densidad`=`poblacion`/`area`)
```

## Precio de compra y alquiler
Estos dos datasets de [compra](https://valencia.opendatasoft.com/explore/dataset/precio-de-compra-en-idealista/table/) y [alquiler](https://valencia.opendatasoft.com/explore/dataset/precio-alquiler-vivienda/table/) nos presentan informaciones similares, que es la media de precios de compra y alquiler en nuestros barrios en los años 2022 y 2010. Ya que son conjuntos muy similares, vamos a adelantarnos al próximo paso y fusionarlos en un único dataset, llamado `precios`.

```{r}
p_compra <- read_delim("./data/precio-de-compra-en-idealista.csv", 
    delim = ";", escape_double = FALSE, col_types = cols(`Geo Point` = col_skip(), 
        `Geo Shape` = col_skip(), Fecha_creacion = col_skip(),`Max_historico (Euros/m2)` = col_skip(), 
        Año_Max_Hist = col_skip()), 
    trim_ws = TRUE)%>%arrange(BARRIO)

p_compra$BARRIO<-factor(p_compra$BARRIO,levels = unique(p_compra$BARRIO))

p_alquiler <- read_delim("./data/precio-alquiler-vivienda.csv", 
    delim = ";", escape_double = FALSE, 
    trim_ws = TRUE, col_types = cols(`Geo Point` = col_skip(), `Geo Shape` = col_skip(), Fecha_creacion = col_skip(),`Max_historico (Euros/m2)` = col_skip(), 
        Año_Max_Hist = col_skip()))%>%arrange(BARRIO)

p_alquiler$BARRIO<-factor(p_alquiler$BARRIO,levels = unique(p_alquiler$BARRIO))

precios<-full_join(p_compra,p_alquiler,by="BARRIO",suffix = c(" de compra"," de alquiler"))
colnames(precios)[colnames(precios) == "BARRIO"] <-"Barrio"

head(precios)
```

## Recibos IBI
Vamos ahora con el dataset [IBI](https://valencia.opendatasoft.com/explore/dataset/rebuts-ibi-2022/table/), que nos da información de los diferentes recibos del IBI (Impuesto sobre Bienes Inmuebles) entre los años 2021 y 2023. Este conjunto nos va a dar una muy buena visión acerca de la actividad del barrio, tanto comercial como cultural, turística, religiosa, industrial, etc. 

Debido a que en ningún momento vamos a tratar con tiempo en este dataset, vamos a eliminar los años haciendo la media de las observaciones de cada barrio durante estos tres años, para así obtener tantas observaciones como barrios, ya que si no habrá conflictos a la hora de unir los datos.

```{r}
ibi <- read_delim("./data/rebuts-ibi-2022.csv", delim = ";", escape_double = FALSE, col_types = cols(ID = col_skip(), geo_shape = col_skip(), geo_point_2d = col_skip(), Año = col_skip()), trim_ws = TRUE)%>%
  arrange(Barrio)%>%
  mutate_at(vars(-all_of(c("Distrito","Barrio"))), ~as.numeric(sub(",",".",.)))

ibi$Barrio<-factor(ibi$Barrio,levels = unique(ibi$Barrio))

# Hacemos la media de las observaciones de cada barrio en los tres año y nos quitamos 2/3 de las observaciones
ibi <- ibi %>% group_by(Barrio) %>%  mutate(across(where(is.numeric), mean, na.rm=TRUE))%>%distinct()

head(ibi)
```

## Bancos por barrio
Por último, vamos con nuestro último conjunto de datos, [barrios](https://valencia.opendatasoft.com/explore/dataset/bancs-en-via-publica-bancos-en-via-publica/table/), que contiene mucha información acerca de la ubicación de las entidades bancarias en nuestra ciudad. Debido a que nosotros solo vamos a tratar con barrios y no con direcciones ni nada similar, hemos decidido que lo más interesante de este conjunto es el número de bancos que podemos encontrar en cada barrio (puede ser un buen indicador de riqueza o pobreza). Guardaremos esta información en un nuevo dataset llamado `num_bancos`.

```{r}
bancos <- read_delim("./data/bancs-en-via-publica-bancos-en-via-publica.csv", 
    delim = ";", escape_double = FALSE, col_types = cols(gid = col_skip(), 
        `Num. Policia` = col_skip(), geo_point_2d = col_skip()), 
    trim_ws = TRUE)%>%arrange(Barrio)

bancos$Barrio%<>%gsub("[0-9] - ","",.)
bancos$Barrio%<>%factor(levels = unique(bancos$Barrio))

num_bancos<-bancos%>%group_by(Barrio)%>%summarize(Num_bancos=n())
head(num_bancos)
```

# Fusionamos todos los dataset

```{r}
df<-vuln%>%full_join(demografico,by="Barrio")%>%full_join(num_bancos,by="Barrio")%>%full_join(precios,by="Barrio")%>%full_join(ibi,by="Barrio")

dim(df)
tail(df)
```

Vemos como a la hora de fusionar todos los datos en un solo dataset, tenemos un problema, y es que contamos con más observaciones de las esperadas. Deberíamos tener un total de 88 observaciones (una por cada barrio), pero en cambio, tenemos 92. Mirando el final del dataset, vemos como efectivamente hay cuatro observaciones que no se corresponden con lo deseado, así que vamos a arreglarlo.

```{r}
# El barrio MONTOLIVET se llama únicamente en el dataset "vuln" y "num_bancos" como MONT-OLIVET
levels(vuln$Barrio)[vuln$Barrio=="MONT-OLIVET"]<-"MONTOLIVET"
levels(num_bancos$Barrio)[num_bancos$Barrio=="MONT-OLIVET"]<-"MONTOLIVET"

# Lo mismo ocurre con la Fonteta de sant lluis y el dataset "ibi
levels(ibi$Barrio)[ibi$Barrio=="FONTETA DE SANT LLUIS"]<-"LA FONTETA S.LLUIS"

# Además, la primera y última observación de barrios no corresponden a ningún barrio, por lo que tenemos que eliminarlas
num_bancos%<>%slice(-c(1,length(num_bancos$Num_bancos)))
```

Vemos como dos de estas incongruencias se debían a la distinta forma de escribir el nombre de los barrios, mientras que las otras dos simplemente se debían a que algún dataset contenía información de barrios desconocidos, lo cual es mejor eliminar directamente.

Una vez solucionado, volvemos a crear el dataset:

```{r}
df<-vuln%>%full_join(demografico,by="Barrio")%>%full_join(num_bancos,by="Barrio")%>%full_join(precios,by="Barrio")%>%full_join(ibi,by="Barrio")

dim(df)
head(df)
```


# Selección de variables
Observando las 69 variables con las que cuenta nuestro conjunto, vemos como claramente hay muchas que no necesitamos. Primero, tenemos todos los códigos de los barrios, que prácticamente cada dataset de los anteriores contaba con una o más variables de este estilo, y con distintos nombres entre sí. Vamos a empezar eliminandolas aplicando expresiones regulares, ya que todas cuentan con una cosa en común, que empiezan por "cod":

```{r}

codigos<-grepl("^[Cc]od",colnames(df))
df%<>%dplyr::select(-which(codigos))
```

Con esto nos hemos quitado un total de 11 variables, pero aún podemos hacer más. También tenemos otra variable redundante, que son los distritos. Como el distrito de compra es el único que no tiene ningún valor perdido, usaremos ese, y además, lo transformaremos en factor:

```{r}
distritos<-colnames(df)[grepl("^DISTRITO|Distrito",colnames(df))]
distritos<-distritos[distritos!="DISTRITO de compra"]

df%<>%dplyr::select(-distritos)

colnames(df)[colnames(df) == "DISTRITO de compra"] <-"Distrito"

df$Distrito<-factor(df$Distrito,levels = unique(df$Distrito))

df%<>%relocate(Barrio,Distrito,Indice_Vuln)
```

Por último, vemos como dentro del dataset IBI tenemos por un lado las variables que indica el número de recibos de un cierto tipo y en otra el importe. Consideramos que nos pueden ser más de utilidad las segundas, y para no ser reduntantes vamos a eliminar las de número de recibos. Además, algunas de estas cuentan con entradas decimales, lo cual es un poco extraño para lo que la variable representa.

```{r}
num<-grepl("^Num\\.",colnames(df))
df%<>%dplyr::select(-colnames(df)[num])
```

Finalmente tenemos nuestro conjunto de datos cargado y liberado de variables innecesarias, vamos a echar un vistazo:

```{r}
dim(df)
```

```{r}
glimpse(df)
```
Tenemos un total de 33 variables numéricas (cuantitativas) y 3 variables de tipo factor (cualitativas). Una vez creado y depurado nuestro conjunto de forma preliminar, vamos a plantear las preguntas que queremos resolver y acabar de poner nuestro dataset a punto.
# Estudio de la vulnerabilidad

## Mapa de barrios vulnerables

```{r carga de librerias y datos mapa}
#primero creamos un mapa con los barrios y distrito
library(sf)
library(ggplot2)

# Lee el archivo GeoJSON
datos_geojson <- st_read("./data/barris-barrios.geojson")
colnames(datos_geojson)[2] <- 'Barrio'

# Selecciono las columnas de df que quiero utilizar en la siguiente gráfica
columnas <- c('Barrio', 'Distrito', 'Indice_Vuln', 'area','poblacion','Densidad')

# Creo el dataframe que voy a utilizar para las gráficas
datos_geojson <- datos_geojson %>%
  full_join(df[columnas], by = 'Barrio')



```

```{r Mapa vulnerabilidad}
library(leaflet)
# Creo los popup del mapa
popups <- paste0("<b>", datos_geojson$Barrio, "</b>", "<hr>", 'poblacion: ', datos_geojson$poblacion)

# Escojo una paleta de colores
pal <- colorFactor(c('red','gray','blue','green'), levels = levels(datos_geojson$Indice_Vuln))

# Creo el mapa
leaflet(data = datos_geojson) %>%
  addTiles() %>%
  addPolygons(fillColor = pal(datos_geojson$Indice_Vuln),
              weight = 1,
              opacity = 1,
              highlightOptions = highlightOptions(color = "white",
                                                  weight = 2,
                                                  bringToFront = TRUE),
              color = 'black',
              fillOpacity = 0.8,
              popup = popups) %>%
  addLegend(data = datos_geojson,
            position = 'bottomright',
            pal = pal, values = ~Indice_Vuln,
            title = 'Leyenda',
            opacity = 1)

```


```{r}
# Creo los popup del mapa
popups <- paste0("<b>", datos_geojson$Barrio, "</b>", "<hr>", datos_geojson$poblacion)

# Escojo una paleta de colores
pal <- colorBin("YlOrRd", domain = datos_geojson$poblacion, bins = 6)

# Creo el mapa
leaflet(data = datos_geojson) %>%
  addTiles() %>%
  addPolygons(fillColor = pal(datos_geojson$poblacion),
              weight = 1,
              opacity = 1,
              highlightOptions = highlightOptions(color = "white",
                                                  weight = 2,
                                                  bringToFront = TRUE),
              color = 'black',
              fillOpacity = 0.8,
              popup = popups) %>%
  addLegend(data = datos_geojson,
            position = 'bottomright',
            pal = pal, values = ~poblacion,
            title = 'Leyenda',
            opacity = 1)
```


```{r}
## Estudio de Vulnerabilidad por distrito
# Creo una tabla de frecuencias, le pongo useNA para que también considere los datos faltantes
table_data <- table(df$Distrito, df$Indice_Vuln, useNA = 'ifany')
df_table <- as.data.frame(as.table(table_data))


colnames(df_table) <- c('Var1', 'Vulnerabilidad', 'Freq')
df_table$Vulnerabilidad <- factor(df_table$Vulnerabilidad, levels = c("Vulnerable", "No Vulnerable", "Pot. Vulnerable"))
```



```{r}
# Crear un gráfico de barras apiladas (mosaic plot)
ggplot(df_table, aes(x = Var1, y = Freq, fill = Vulnerabilidad)) +
  geom_bar(stat = "identity") +
  labs(title = "Barras apiladas") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(y="Vulnerabilidad", x="Distrito") 
```

```{r}
# Creo una tabla con las columnas Dist|Barrio|Vuln|poblacion
columnas <- c('Barrio','Distrito','Indice_Vuln','poblacion')
df_table2 <- df %>%
  dplyr::select(columnas) %>%
  group_by(Distrito, Indice_Vuln) %>%
  summarise(across(poblacion, list(suma=~sum(., na.rm = T))))

df_table2$Indice_Vuln <- factor(df_table2$Indice_Vuln, levels = c("Vulnerable", "No Vulnerable", "Pot. Vulnerable"))

ggplot(df_table2, aes(x = Distrito, y = poblacion_suma, fill = Indice_Vuln)) +
  geom_bar(stat = "identity") +
  labs(title = "Barras apiladas") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(y="Vulnerabilidad", x="Distrito") 

```



```{r}
# Mosaico en porcentajes
df_table2 <- df_table
suma <- df_table %>%
  group_by(Var1) %>%
  summarise(across(ends_with('eq'), list(suma=sum)))
for (i in 1:length(df_table$Var1)) {
  index <- which(suma$Var1 == df_table$Var1[i])
  df_table2$Freq[i] <- df_table$Freq[i] / suma$Freq_suma[index]
}

ggplot(df_table2, aes(x = Var1, y = Freq, fill = Vulnerabilidad)) +
  geom_bar(stat = "identity") +
  labs(title = "Mosaic Plot") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(y="Vulnerabilidad (%)", x="Distrito") 
```


# Distribución poblacional
```{r}
# vamos a hacer un mapa donde se muestre la cantidad de gente que vive en cada barrio
distritos_geojson <- st_read("./data/districtes-distritos.geojson")
# El nombre de uno de los distritos no coincide -> lo cambio a mano
distritos_geojson$nombre[distritos_geojson$nombre == 'POBLATS DE L\'OEST'] <- "POBLATS LOEST"
colnames(distritos_geojson)[2] <- 'Distrito'

# Selecciono las columnas que quiero utilizar en la siguiente gráfica
columnas <- c('Barrio', 'Distrito', 'Indice_Vuln', 'area','poblacion')

# Creo el dataframe que voy a utilizar para las gráficas
distritos_geojson <- distritos_geojson %>%
  full_join(df[columnas], by = 'Distrito')

# Creo una variable nueva que sea la poblacion del distrito donde se encuentra cada barrio
distritos_geojson <- distritos_geojson %>%
  group_by(Distrito) %>%
  mutate(Pob_dist = sum(poblacion, na.rm = T))


```


```{r}
#    PRIMERO LA DISTRIBUCÓN DE POBLACIÓN


# Creo los popup del mapa
popups_dist <- paste0("<b>", distritos_geojson$Distrito, "</b>", "<hr>", paste('Población: \n', distritos_geojson$Pob_dist))

# Escojo una paleta de colores
pal <- colorBin("YlOrRd", domain = distritos_geojson$Pob_dist, bins = 6)

# Creo el mapa
leaflet(data = distritos_geojson) %>%
  addTiles() %>%
  addPolygons(fillColor = pal(distritos_geojson$Pob_dist),
              weight = 1,
              opacity = 1,
              highlightOptions = highlightOptions(color = "white",
                                                  weight = 2,
                                                  bringToFront = TRUE),
              color = 'black',
              fillOpacity = 0.8,
              popup = popups_dist) %>%
  addLegend(data = distritos_geojson,
            position = 'bottomright',
            pal = pal, values = ~Pob_dist,
            title = 'Leyenda',
            opacity = 1)


```

```{r}
# Ahora para barrios

# Creo los popup del mapa
popups <- paste0("<b>", distritos_geojson$Barrio, "</b>", "<hr>", paste('Población: \n', distritos_geojson$poblacion))

# Escojo una paleta de colores
pal <- colorBin("YlOrRd", domain = distritos_geojson$poblacion, bins = 6)

# Creo el mapa
leaflet(data = distritos_geojson) %>%
  addTiles() %>%
  addPolygons(fillColor = pal(distritos_geojson$poblacion),
              weight = 1,
              opacity = 1,
              highlightOptions = highlightOptions(color = "white",
                                                  weight = 2,
                                                  bringToFront = TRUE),
              color = 'black',
              fillOpacity = 0.8,
              popup = popups) %>%
  addLegend(data = distritos_geojson,
            position = 'bottomright',
            pal = pal, values = ~poblacion,
            title = 'Leyenda',
            opacity = 1)


```


Ha sido necesiaro realizar alguna corrección de los *inputs* del *dataset*. En el caso de la variable `risc_pobre`, el valor introducido para el barrio de Benimaclet, distaba 43.76 veces el rango intercuartílico de la mediana de la distribución. Por tanto, se ha considerado un error de *input* y se le ha selecccionado un nuevo valor. Para ello, se ha tenido en cuenta que el análisis que se ha realizado ha sido mediante box-plots, donde se diferenciaban las distribuciones en función de la varaible categórica `Indice_Vuln`. Por tanto, para que no altere esta gráfica, el valor de la observación de Benimaclet se ha sustituido por la mediana correspondiente a la distribución con su misma vulnerabilidad.
```{r}
# Corrección de algunos outliers
ggplot(df, aes(x = Indice_Vuln, y = risc_pobre)) + geom_boxplot() + ggtitle('Distribución risc_pobre antes de tratar')

risc_pobre_filtrada <- df %>%
  filter(Indice_Vuln == 'Vulnerable') %>%
  filter(risc_pobre < 100) %>%
  dplyr::select(risc_pobre)
df$risc_pobre[df['Barrio'] == 'BENIMACLET'] <- median(risc_pobre_filtrada[[1]])

ggplot(df, aes(x = Indice_Vuln, y = risc_pobre)) + geom_boxplot() + ggtitle('Distribución risc_pobre después de tratar')
```


```{r}
# me quedo con las columnas numéricas + Indice_Vuln de df
columnas_numericas <- df %>%
  select_if(is.numeric) %>%
  colnames()
columnas_numéricas <- c(columnas_numericas, 'Indice_Vuln')
columnas_numéricas <- df[columnas_numéricas]

# Muestro un box-plot para cada variable numérica diferenciando 4 distribuciones en función de la vulnerabilidad
for (i in colnames(columnas_numéricas)) {
  p<-ggplot(columnas_numéricas, aes(x = Indice_Vuln, y =.data[[i]])) +
    geom_boxplot() + 
    ggtitle(paste("Relación ", i))
  print(p)
}

```

Una cosa que me ha interesado es ver si la diferencia del aumeto de precio de compra en función de la vulnerabilidad
```{r}

# Seleccionamos la parte del dataframe que queremos analizar
precio_compra <- data.frame(df$Barrio, df$Indice_Vuln, df$`Precio_2022 (Euros/m2) de compra`, df$`Precio_2010 (Euros/m2) de compra`)
colnames(precio_compra) <- c('Barrio', 'Indice_Vuln', '2010', '2022')

# Lo ponemos tidy
precio_compra <- precio_compra %>%
  pivot_longer('2010':'2022', names_to = 'Año', values_to = 'Precio')

# Hacemos un box-plot
ggplot(precio_compra, aes(x = Indice_Vuln)) + geom_boxplot(aes(y = Precio, col = Año))
```

## Anális con las columnas catergóricas
```{r}
library('corrplot')
columnas_categoricas <- df %>%
  select_if(is.numeric) %>%
  colnames()
columnas_categoricas <- df[!(colnames(df) %in% columnas_categoricas)] 
#tabla de contingencia

# columnas_categoricas %>%
#   cor(use="complete.obs") %>%
#   corrplot(type="lower", diag=FALSE)

```





###############
```{r}
library(GGally)
library(dplyr)

df_numeric <- select_if(df, is.numeric)

# Correlación de Pearson
cor_pearson <- cor(df_numeric, method = "pearson", use = "complete.obs")

# Correlación de Spearman
cor_spearman <- cor(df_numeric, method = "spearman", use = "complete.obs")
```


```{r}
library(tidyr)
# Pearson
cor_pearson_long <- as.data.frame(cor_pearson) %>%
  rownames_to_column(var = "Variable1") %>%
  gather(key = "Variable2", value = "Correlation", -Variable1)

# Filtrar las correlaciones mayores a 0.8 y menores de 1 (para positivos y negativos)
strong_correlations <- cor_pearson_long %>%
  filter(abs(Correlation) > 0.8, abs(Correlation) < 1) %>%
  filter(!duplicated(t(apply(.[, c("Variable1", "Variable2")], 1, sort))))

# Spearman
cor_spearman_long <- as.data.frame(cor_spearman) %>%
  rownames_to_column(var = "Variable1") %>%
  gather(key = "Variable2", value = "Correlation", -Variable1)

# Filtrar las correlaciones mayores a 0.8 y menores de 1 (para positivos y negativos)
strong_correlations_spearman <- cor_spearman_long %>%
  filter(abs(Correlation) > 0.8, abs(Correlation) < 1) %>%
  filter(!duplicated(t(apply(.[, c("Variable1", "Variable2")], 1, sort))))

print(strong_correlations)
print(strong_correlations_spearman)
```


```{r}
library(corrplot)

corrplot(mar = c(1, 0, 1, 0), cor_spearman, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45,
         tl.cex = 0.3,
         cl.lim = c(-1, 1), cl.cex = 0.3, cl.ratio = 0.1,
         addCoef.col = "black", number.cex = 0.25, coef.cex = 0.3,
         col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
         main = "Correlaciones Spearman")
```
En este grafico de correlaciones de Spearman podemos observar visualmente como de correlacionadas están todas nuestras variables entre todas ellas, ya que estamos representando las correlaciones de spearman, éstas no tienen por qué ser relaciones lineales. Ya que no se puede observar de forma clara las relaciones, realizaremos un diagrama de Red donde poder ver solo las más grandes tanto de Pearson como de Spearman.

Primero, calcularemos las correlaciones de spearman que no estén en pearson, ya que una correlación alta de pearson implica tambien una correlación alta de spearman, pero saber que es de pearson nos da una información mayor a si sabemos que es de spearman, ya que nos dice que la relación es lineal.
```{r}
# Unir ambos data frames
all_correlations <- rbind(strong_correlations, strong_correlations_spearman)

# Crear una función para ordenar las variables (para que las parejas sean consistentes en orden)
order_pair <- function(df) {
  df %>%
    rowwise() %>%
    mutate(Variable1 = pmin(Variable1, Variable2),
           Variable2 = pmax(Variable1, Variable2)) %>%
    ungroup()
}

# Ordenar pares en ambos data frames
all_correlations_order <- order_pair(all_correlations)
strong_correlations_order <- order_pair(strong_correlations)

# Encontrar las diferencias
spearman_not_in_pearson <- anti_join(all_correlations_order, strong_correlations_order, 
                                 by = c("Variable1", "Variable2"))

```

```{r}
library(igraph)
library(dplyr)

set.seed(137)

# Asegúrate de que spearman_not_in_pearson tenga las mismas columnas
spearman_not_in_pearson <- spearman_not_in_pearson %>%
  dplyr::select(Variable1, Variable2, Correlation) %>%
  mutate(Method = "Spearman")

strong_correlations <- strong_correlations %>%
  dplyr::select(Variable1, Variable2, Correlation) %>%
  mutate(Method = "Pearson")

# Combina los datos de Pearson y Spearman
combined_correlations <- rbind(strong_correlations, spearman_not_in_pearson)

# Crea un grafo a partir de los datos combinados
graph_data <- graph_from_data_frame(combined_correlations, directed = FALSE)

# Ajustar atributos del nodo
V(graph_data)$color <- "skyblue"
V(graph_data)$size <- 6
V(graph_data)$frame.color <- "black"

# Ajustar atributos de las aristas
cor_min <- 0.8
cor_max <- 1.0
width_min <- 1
width_max <- 5
E(graph_data)$width <- ifelse(E(graph_data)$Method == "Pearson",
                              (abs(E(graph_data)$Correlation) - cor_min) / (cor_max - cor_min) * (width_max - width_min) + width_min,
                              2) # Grosor fijo para Spearman

E(graph_data)$lty <- ifelse(E(graph_data)$Method == "Pearson", 1, 3)

# Elegir un layout
layout <- layout_with_fr(graph_data)

# Dibujar el gráfico
par(mar = c(0, 0, 1.5, 1))
plot(graph_data, layout = layout, vertex.label.color = "black", vertex.label.cex = 0.7,
     vertex.label.dist = 1.2,
     edge.label = NA,
     edge.color = ifelse(E(graph_data)$Correlation < 0, "red", "gray"),
     main = "Red de Correlaciones")

# Añadir leyenda
legend("topright",
       legend = c("Pearson > 0.8", "Pearson < -0.8", "Spearman > 0.8", "Spearman < -0.8"),
       col = c("gray", "red", "gray", "red"),
       lty = c(1,1,3,3),
       cex = 0.8)

```



Vemos que la correlación entre el metro cuadrado de compra y de alquiler es alta pero no tanto como podríamos esperar.
```{r}
cor_pearson_precios <- cor(df$`Precio_2022 (Euros/m2) de compra`, df$`Precio_2022 (Euros/m2) de alquiler`, method = "pearson", use = "complete.obs")
cor_pearson_precios

cor_spearman_precios <- cor(df$`Precio_2022 (Euros/m2) de compra`, df$`Precio_2022 (Euros/m2) de alquiler`, method = "spearman", use = "complete.obs")
cor_spearman_precios
```

Esto implica que, en algunos barrios, el precio de compra puede ser relativamente alto mientras que el precio de alquiler puede ser bajo, o viceversa. Puede ser util si queremos buscar una vivienda para alquilar o comprar ya que vemos que barrios son mejores para cada caso. Al añadirle como tercera variable el indice de vulnerabilidad pintando los puntos, podemos observar que hay una relación entre el indice de vulnerabilidad y el precio del metro cuadrado de compra pero no así con el precio del metro cuadrado de alquiler
```{r}
ggplot(df, aes(x = `Precio_2022 (Euros/m2) de compra`, y = `Precio_2022 (Euros/m2) de alquiler`)) +
    geom_point() +
    labs(title = "Relación entre Precio por Metro Cuadrado de Compra y Alquiler",
         x = "Precio por Metro Cuadrado de Compra",
         y = "Precio por Metro Cuadrado de Alquiler")

ggplot(df, aes(x = `Precio_2022 (Euros/m2) de compra`, y = `Precio_2022 (Euros/m2) de alquiler`, color=Indice_Vuln)) +
    geom_point() +
    labs(title = "Relación entre Precio por Metro Cuadrado de Compra y Alquiler",
         x = "Precio por Metro Cuadrado de Compra",
         y = "Precio por Metro Cuadrado de Alquiler")
```




#Realizamos un test chi-cuadrado para conocer si hay una relacion clara entre el distrito en el que se encuentra el barrio y el indice de vulnerabilidad de ese barrio. El resultado es un p-value de 0.01966, por lo que podemos rechazar la hipotesis nula de que las variables son independientes a un nivel de significancia del 0.05 (nivel de significancia estándar).
```{r}
chisq_result_distrito <- chisq.test(df$Distrito, df$Indice_Vuln)
print(chisq_result_distrito)
```
```{r}
library(ggmosaic)
ggplot(data = df) +
    geom_mosaic(aes(weight = 1, x = product(Distrito), fill = Indice_Vuln)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, size = rel(0.8))) +
    labs(title = "Mosaic Plot de Distrito vs Indice_Vuln", x = "Distrito", y = "Índice de Vulnerabilidad")

```

#Analisis Univariante

##Frecuencias en nuestras variables categoricas
```{r}
frecuencias_distrito <- table(df$Distrito)
df_frecuencias_distrito <- as.data.frame(frecuencias_distrito)
names(df_frecuencias_distrito) <- c("Distrito", "Frecuencia")

ggplot(df_frecuencias_distrito, aes(x = Distrito, y = Frecuencia)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),plot.title = element_text(hjust = 0.5)) +
  labs(title = "Frecuencia de Distritos", x = "Distrito", y = "Frecuencia")

frecuencias_Indice_Vuln <- table(df$Indice_Vuln)
df_frecuencias_Indice_Vuln <- as.data.frame(frecuencias_Indice_Vuln)
names(df_frecuencias_Indice_Vuln) <- c("Indice_Vuln", "Frecuencia")

ggplot(df_frecuencias_Indice_Vuln, aes(x = Indice_Vuln, y = Frecuencia)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=8),plot.title = element_text(hjust = 0.5)) +
  labs(title = "Frecuencia de Indice Vulnerabilidad", x = "Distrito", y = "Frecuencia")
```
```{r}
ggplot(df, aes(x = Densidad)) +
  geom_histogram(binwidth = 10,   # Ajusta esto según lo que necesites
                 fill = "blue", 
                 color = "black") +
  labs(title = "Histograma de Densidad",
       x = "Densidad",
       y = "Frecuencia") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

summary(df$Densidad)
```












